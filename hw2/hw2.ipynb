{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2_autobreya.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolmash/autobreya/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVp0qWBOoIly"
      },
      "source": [
        "# Извлечение коллокаций + NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ3A70HOoIlz"
      },
      "source": [
        "В качестве корпуса я использовала отзывы с Amazon на одежду и аксессуары "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLZUNIfur0-x",
        "outputId": "6ca0ee5a-f36c-4b51-eb64-38aa64038024"
      },
      "source": [
        "!wget https://www.dropbox.com/s/awgoua1rp1i871d/Clothing_%26_Accessories.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-21 00:11:02--  https://www.dropbox.com/s/awgoua1rp1i871d/Clothing_%26_Accessories.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.18, 2620:100:6032:18::a27d:5212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/awgoua1rp1i871d/Clothing_%26_Accessories.txt [following]\n",
            "--2020-12-21 00:11:02--  https://www.dropbox.com/s/raw/awgoua1rp1i871d/Clothing_%26_Accessories.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc21294feb9181db412bfa73479c.dl.dropboxusercontent.com/cd/0/inline/BFdFZyzsPZ8tvcPyweXszPoB7s-BLR_Vq143edMBrHs-i8S-xlkUBUmrgiPpy8kFFYi1xdM477nbYzuwFLtwD9xZcnvddSagzR1gg-Po764DSwbafHf0k_VBufVODH_PcbU/file# [following]\n",
            "--2020-12-21 00:11:03--  https://uc21294feb9181db412bfa73479c.dl.dropboxusercontent.com/cd/0/inline/BFdFZyzsPZ8tvcPyweXszPoB7s-BLR_Vq143edMBrHs-i8S-xlkUBUmrgiPpy8kFFYi1xdM477nbYzuwFLtwD9xZcnvddSagzR1gg-Po764DSwbafHf0k_VBufVODH_PcbU/file\n",
            "Resolving uc21294feb9181db412bfa73479c.dl.dropboxusercontent.com (uc21294feb9181db412bfa73479c.dl.dropboxusercontent.com)... 162.125.82.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to uc21294feb9181db412bfa73479c.dl.dropboxusercontent.com (uc21294feb9181db412bfa73479c.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 372917207 (356M) [text/plain]\n",
            "Saving to: ‘Clothing_&_Accessories.txt’\n",
            "\n",
            "Clothing_&_Accessor 100%[===================>] 355.64M  17.9MB/s    in 23s     \n",
            "\n",
            "2020-12-21 00:11:27 (15.4 MB/s) - ‘Clothing_&_Accessories.txt’ saved [372917207/372917207]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE_k8YH7oIl0"
      },
      "source": [
        "import re\n",
        "\n",
        "filename = \"Clothing_&_Accessories.txt\"\n",
        "\n",
        "reviews = []\n",
        "with open(filename) as f:\n",
        "    f = f.readlines()\n",
        "for l in f:\n",
        "    l = l.strip()\n",
        "    if re.match('review/text:', l):\n",
        "        start_ind = len('review/text: ')\n",
        "        reviews.append(l[start_ind:])\n",
        "reviews = list(set(reviews))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXlKZuTwoIl2"
      },
      "source": [
        "## 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GTOI4uvoIl2"
      },
      "source": [
        "**Первый способ:**\n",
        "\n",
        "Вычитав где-то 100 отзывов, я заметила, что в текстах часто повторяются одни и те же фразы/выражения, из которых достаточно легко с помощью правил вытащить сущность товара. Это либо оценочные суждения, либо констатация факта (я заказал, купил):\n",
        "- *I bought/ purchaced/ received this (dress)*\n",
        "- *Love/enjoy this (shirt)*\n",
        "- различные словоформы лексемы *wear (I wear this jeans every day/I have worn this ring for a year)*\n",
        "- *what a (beautiful dress)*\n",
        "- *one of the best (pair of pants) I have had*\n",
        "- this is (comfortable jeans)\n",
        "\n",
        "В силу того, что в такой категории текстов используется упрощенная лексика, отсутствие каких-либо художественных средств, действительно, такие предложения могут встречаться почти в каждом отзыве. Поэтому будем использовать их как шаблоны bootstrapping и будем находить названия товары с помощью задания правил вручную.\n",
        "\n",
        "Минус такого подхода в том, что, естесственно, такие шаблонные фразы есть далеко не во всех отзывах, и какую-то часть информации мы упустим. Очень часто также в отзывах эксплицитно не называется товар, а заменяется местоимением it, но при этом такая сущность все равно будет подходить под шаблон, и нам надо будет все подчищать, в зависимости от части речи. И еще есть проблема с размытыми границами. Словит ли шаблон употребление (blue jeans вместо beatiful blue jeans)? А если нет, являются ли два этих упоминания товаров различными для нашего отчета? Это проблема в принципе актуальна для всех методов, и такие вещи нужно заранее обговаривать с заказчиком, если речь идет о реальной задаче\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OfaFWk9oIl2"
      },
      "source": [
        "**Второй способ:**\n",
        "\n",
        "Помимо того, что у нас есть текст самого отзыва, также у каждого товара есть заголовок. Конкретно для корпуса с отзывами на одежду и аксессуары, скорее всего в названии будет не сам item одежды, но и цвет, стиль, бренд, узор. Поэтому просто проверять вхождение заголовка в отзыв - плохая идея. Здесь можно использовать векторные представления, например, word2vec. Чтобы модель сконцентрировалась именно на нашей задаче, можно с помощью библиотеки gensim обучить свой word2vec на всех отзывах и на всех названиях товаров. И затем для каждого названия (желательно убрать оттуда с помощью морфологического анализа все, что не существительное, еще и неодушевленное) искать наиболее близкие слова в отзывах. \n",
        "\n",
        "Если не использовать заголовки (это возможно только усложнит задачу, потому что некоторые наименования товаров очень длинные, содержат много ненужной информации), то можно извлечь с сайта любого интернет-магазина категории товаров (одежды и аксессуары), и искать похожие слова на них. Но тогда мы не будем знать какой отзыв соответствует какой категории. Будет больше вычислений.\n",
        "\n",
        "Минусы: word2vec может не сделать чуда, и не находить что-то внятно похожее на название/категорию товаров. Остается проблема с отзывами, где нет прямого обозначения товара, есть только it. Сложно с точки зрения вычислимости."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh4AiR5OoIl2"
      },
      "source": [
        "**Третий способ:**\n",
        "\n",
        "Использовать BERT для задачи NER или просто нейросеть (biLSTM-CRF), которая будет предсказывать NE в тексте отзыва. Самая большая проблема в том, что нужен какой-то датасет для обучения. В принципе можно использовать простые методы, как выше, чтобы нагенерировать примеры, подчистить, и подать на вход модели, которая сделает все за нас. Можно также добавить какие-то признаки, например, pos-разметку (сущность скорее всего - существительное),  синтаксическую информацию (из шаблонов выше видно, что сущность чаще всего либо подлежащее либо прямое дополнение). \n",
        "\n",
        "Минусы: как я уже написала, нужны размеченные данные. Обучать что-либо достаточно долго, и надо пробовать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j7cTJBGoIl2"
      },
      "source": [
        "**Четвертый способ**: \n",
        "\n",
        "Довольно примитивный, но мне внезапно пришла в голову эта идея и я решила добавить. Для каждого отзыва указан id продукта, для которого он написан. А что, если сгруппировать отзывы по каждому продукту (а лучше вообще для каждой категории товаров, если это как-то закодировано в id). Получится, что для каждого товара у нас имеется один большой отзыв, и внутри него могут быть совершенно разные, написанные разными людьми упоминания одного и того же товара. Среди них можно посмотреть на биграммы с высокой совместной встречаемостью, или для начала просто на самые частотные существительные. Я думаю, что какой-либо предмет одежду/аксессуаров имеет достаточно ограниченное количество слов, которым их можно назвать. А уже от полученных существительных, или биграмм, можно искать в помощью синтаксической разметки зависящие от них единицы (какую-то целую составляющую в дереве. Например, если мы нашли с помощью частых биграмм christmas socks, то обязательно найдем nice christmas socks).\n",
        "\n",
        "Минусы: нужно все просматривать глазами, все чистить от лишнего. Использование нетривиальной синтаксической разметки, тем более предложения могут быть грамматически неполными, если это какой-то короткий отзыв, и парсер на них сломается. Уникальные упоминания товаров (а вдруг такие будут) не будут являться частотными, а значит, не попадут в наше рассмотрение. С другой стороны, топ самых редких слов (особенно только существительных) просмотреть - не очень большая проблема."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwzhzbMEoIl2"
      },
      "source": [
        "## 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jta-Hw8HoIl3"
      },
      "source": [
        "Я буду реализовывать первый метод с помощью вручную заданных правил/шаблонов\r\n",
        "\r\n",
        "В задании указан совет делать это через natasha/yargy, что очень странно, поскольку он написан для русского языка (внутри него лежит pymorphy, можно, наверное писать простые правила для английского и без функционала морфологии, но кажется, это не очень рационально). Поэтому я использую похожий [инструмент](https://spacy.io/usage/rule-based-matching#matcher) из spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51I3k7Gilfol"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.matcher import Matcher"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6jdo3qIhjzI"
      },
      "source": [
        "Шаблоны пишутся с помощью ограничений на токены (вернее, на то, как их разбирает spacy): на их леммы, морфологические теги и т.д. \r\n",
        "\r\n",
        "Исходя из тех частотных фраз, что я выделила ранее, создадим три шаблона:\r\n",
        "1. название продукта является прямым объектом глаголов, которые выражают оценочное суждение (*love, enjoy, like*) или глаголов, доказывающих факт покупки товара (*buy, purchase, get*). У названия продукта также может быть артикль и различные модификаторы в виде прилагательных и существительных\r\n",
        "2. чисто оценочные фразы типа *this is a nice shirt* или *it is the best socks*. Здесь название продукта входит в составное сказуемое, состоящее также из вспомогательного глагола *be*. Также учитывается неограниченное кол-во модификаторов\r\n",
        "3. Фразы *what a beautiful dress!*. Здесь название продукта является непосредственно корнем дерева, а *what* - его очередным детерминантом.\r\n",
        "\r\n",
        "Понятно, что этими тремя шаблонами, конечно, задача не ограничивается. Но эти мне показались самыми частотными и легко выделяемыми."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmsNl10RyMi7"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "matcher = Matcher(nlp.vocab)\r\n",
        "pattern1 = [{'LEMMA': {\"IN\": ['buy','receive','purchase','order','love','enjoy','like','wear' ]}},\r\n",
        "           {'POS': 'DET', 'OP': '?'},\r\n",
        "           {'DEP': 'amod', 'OP': '*'},\r\n",
        "           {'POS': 'NOUN', 'OP': '*'},\r\n",
        "           {'DEP': 'dobj', 'POS': 'NOUN'}]\r\n",
        "matcher.add('verb+dobj', None, pattern1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPB979NAOXVo"
      },
      "source": [
        "pattern2 = [{'LEMMA': {'IN': ['this', '-PRON-']}, 'DEP': 'nsubj'},\r\n",
        "            {'LEMMA': 'be', 'POS': 'AUX', 'TAG': 'VBZ'},\r\n",
        "            {'POS': 'DET', 'OP': '?'},\r\n",
        "            {'DEP': 'amod', 'OP': '*'},\r\n",
        "            {'POS': 'NOUN', 'OP': '*'},\r\n",
        "            {'DEP': 'attr', 'POS': 'NOUN'}\r\n",
        "            ]\r\n",
        "matcher.add('this+attr', None, pattern2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPHtBnsASP15"
      },
      "source": [
        "pattern3 = [{'LEMMA': 'what', 'DEP': 'det'},\r\n",
        "            {'POS': 'DET', 'OP': '?'},\r\n",
        "            {'DEP': 'amod', 'OP': '*'},\r\n",
        "            {'POS': 'NOUN', 'OP': '*'},\r\n",
        "            {'DEP': 'ROOT', 'POS': 'NOUN'}\r\n",
        "            ]\r\n",
        "matcher.add('what+root', None, pattern3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bu6mBSljTD9"
      },
      "source": [
        "Посмотрим, что за части отзывов выделяются по этим шаблонам:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGixpsJfzJLp",
        "outputId": "814ff467-79dd-412f-ad89-8420040ab0d4"
      },
      "source": [
        "for rev in reviews[:200]:\r\n",
        "  doc = nlp(rev)\r\n",
        "  matches = matcher(doc)\r\n",
        "  for match_id, start, end in matches:\r\n",
        "    span = doc[start:end]\r\n",
        "    print(span.text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bought this wallet\n",
            "loves magic wallets\n",
            "likes this wallet\n",
            "it's good quality\n",
            "order a larger one\n",
            "order my size\n",
            "Love this shirt\n",
            "he's a Hawaiian shirt kind\n",
            "LOVES this one\n",
            "bought this belt\n",
            "purchased a reversible belt\n",
            "bought this shirt\n",
            "wearing a black shirt\n",
            "bought a pair\n",
            "ordered medium size\n",
            "wears Levis\n",
            "like these panties\n",
            "This is a great suit\n",
            "loves these pants\n",
            "buying this product\n",
            "bought this bag\n",
            "buy another one\n",
            "order a size\n",
            "This is a nice umbrella\n",
            "loved these socks\n",
            "love the cat eyes\n",
            "love the pocket\n",
            "this is a welcome addition\n",
            "it is the perfect thing\n",
            "bought several pairs\n",
            "worn my turtleneck\n",
            "worn mine\n",
            "bought a hook\n",
            "This is a wonderful top\n",
            "This is a product\n",
            "she's a lab\n",
            "ordered this shirt\n",
            "buy XXXL\n",
            "purchase another online\n",
            "bought the long length\n",
            "purchased the mediums\n",
            "love this swimsuit\n",
            "buy this brand\n",
            "liked this one\n",
            "received this box\n",
            "like the variety\n",
            "love the fit\n",
            "bought this shaper\n",
            "buy a drill\n",
            "purchased the item\n",
            "Bought this visor\n",
            "It is scratcy\n",
            "this is the product\n",
            "like the bow\n",
            "bought a pair\n",
            "It's reflective silver\n",
            "Bought several pair\n",
            "love the way\n",
            "bought a pair\n",
            "purchased this hat\n",
            "it's the one\n",
            "loves the quality\n",
            "wear the sign\n",
            "wear my thong\n",
            "wears robes\n",
            "like a lap blanket\n",
            "This is a great vest\n",
            "It's the gift\n",
            "buy this brand\n",
            "wearing this product\n",
            "LOVE THIS PRODUCT\n",
            "This is the only bra\n",
            "buy the colors\n",
            "received my saber\n",
            "purchased these jeans\n",
            "buy the long sleeve version\n",
            "It is good quality\n",
            "ordering another pair\n",
            "purchased this costume\n",
            "wore their costumes\n",
            "It is a great buy\n",
            "love these tights\n",
            "it is a great product\n",
            "It is an underwired garment\n",
            "bought a stocking cap\n",
            "Love these shirts\n",
            "This is a great costume\n",
            "order the size\n",
            "bought this shirt\n",
            "order meals\n",
            "it's limits\n",
            "buying things\n",
            "worn the nude\n",
            "ordering the right size\n",
            "worn the same pair\n",
            "This is the real thing\n",
            "Love the length\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeMfkE6ikM9j"
      },
      "source": [
        "Выделяется примерно то, что и планировалось. И даже если у названия продукта есть какие-то модификации в виде прилагательных или других существительных типа *leather*, они тоже были выделены. Естественно, не все примеры идеальны - например, шаблон цепляет фразы про размер продукта, про его форму/качество, поскольку все эти характеристики также представлены в виде существительных (*size, style, length, quality, cotton*). Также стоит сделать скидку на то, что не все разборы spacy идеальны, а в шаблонах есть достаточно сильная привязка к морфосинтаксическим тегам."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LkZvzvinUix"
      },
      "source": [
        "По сути, из большого отзыва мы научились вытаскивать микро-предложение, поскольку все, что подходит под шаблон, грамматически им является. Соответственно, из этих вычлененных частей будет легко достать само название продукта, опять же с помощью синтаксических тегов spacy, которые будут корректными в силу полноты предложения.\r\n",
        "\r\n",
        "Было бы удобно прописывать в правилах, как в yargy, какое из правил относится к какой сущности, и сразу их маркировать, тогда можно было бы отдельно доставать глагол, отдельно модификации, отдельно - сам продукт. Но, к сожалению, такой функционал не предусмотрен.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbS1yskF1cFW"
      },
      "source": [
        "Будем работать с каждым шаблоном по отдельности, потому что в каждом шаблоне само название продуктов имеет разную синтаксическую роль.\r\n",
        "\r\n",
        "В первом шаблоне продуктом является прямой объект. У него также может быть важный модификатор, без которого единственное существительное не является названием продуктом. Такие модификаторы spacy помечает как compound (см.ниже в дереве *phone cover* - полное название продукта). При наличии, его тоже будем извлекать. Модификации в виде прилагательных не будем извлекать, потому что они часто оценочные. Их проанализируем ниже, когда будем смотреть на соседей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWqcaHbIoIl7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "ee446356-2a04-4433-dad8-72d1beab37a8"
      },
      "source": [
        "from spacy import displacy\r\n",
        "from IPython.core.display import display, HTML\r\n",
        "\r\n",
        "doc1 = nlp('bought this phone cover')\r\n",
        "html = displacy.render([doc1], style=\"dep\")\r\n",
        "display(HTML(html))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1c954b94621244d1a54509e90dd50bcc-0\" class=\"displacy\" width=\"750\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">bought</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">this</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">phone</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">cover</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1c954b94621244d1a54509e90dd50bcc-0-0\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1c954b94621244d1a54509e90dd50bcc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1c954b94621244d1a54509e90dd50bcc-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1c954b94621244d1a54509e90dd50bcc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1c954b94621244d1a54509e90dd50bcc-0-2\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1c954b94621244d1a54509e90dd50bcc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,266.5 L583.0,254.5 567.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au3pBvPz4YLq"
      },
      "source": [
        "В рамках задания я возьму не полный датасет из почти 600 000 отзывов, а только 100 000. Matcher из spacy работает очень долго, а этого объема текстов должно хватить для ранжирования"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7-yprtaoOnK"
      },
      "source": [
        "matcher1 = Matcher(nlp.vocab)\r\n",
        "matcher1.add('verb+dobj', None, pattern1)\r\n",
        "\r\n",
        "matched_to_pattern1 = []\r\n",
        "\r\n",
        "for rev in reviews[:100000]:\r\n",
        "  doc = nlp(rev)\r\n",
        "  matches = matcher1(doc)\r\n",
        "  for match_id, start, end in matches:\r\n",
        "    span = doc[start:end]\r\n",
        "    matched_to_pattern1.append(span.text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk6FGk5jpYBg",
        "outputId": "6a499abd-46ec-41fc-9a03-224cba757b6a"
      },
      "source": [
        "print(len(matched_to_pattern1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmDYdhRb-dai"
      },
      "source": [
        "В список products будем записывать вытянутые упоминания товаров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLBC-TzZ21FD"
      },
      "source": [
        "matcher1_to_products = Matcher(nlp.vocab)\r\n",
        "pattern1_products = [{'DEP': 'compound', 'POS': 'NOUN', 'OP': '*'},\r\n",
        "                     {'DEP': 'dobj', 'POS': 'NOUN'}]\r\n",
        "matcher1_to_products.add('dobj', None, pattern1_products)\r\n",
        "\r\n",
        "products = []\r\n",
        "for micro_sent in matched_to_pattern1:\r\n",
        "  doc = nlp(micro_sent)\r\n",
        "  matches = matcher1_to_products(doc)\r\n",
        "  for match_id, start, end in matches:\r\n",
        "    span = doc[start:end]\r\n",
        "    if span.text not in products:\r\n",
        "      products.append(span.text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-yE1KyZ0IP3"
      },
      "source": [
        "Проделаем то же самое со вторым шаблоном, где искомое название товара является частью составного сказуемого (тег attr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgXlf41m5GAS"
      },
      "source": [
        "matcher2 = Matcher(nlp.vocab)\r\n",
        "matcher2.add('this+attr', None, pattern2)\r\n",
        "\r\n",
        "matched_to_pattern2 = []\r\n",
        "\r\n",
        "for rev in reviews[:100000]:\r\n",
        "  doc = nlp(rev)\r\n",
        "  matches = matcher2(doc)\r\n",
        "  for match_id, start, end in matches:\r\n",
        "    span = doc[start:end]\r\n",
        "    matched_to_pattern2.append(span.text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbXAf4cR5r-p",
        "outputId": "ea12a6ff-1138-4eb1-a9e2-9ce7af8e49ae"
      },
      "source": [
        "print(len(matched_to_pattern2))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqvy5jE65ms8"
      },
      "source": [
        "matcher2_to_products = Matcher(nlp.vocab)\r\n",
        "pattern2_products = [{'DEP': 'compound', 'POS': 'NOUN', 'OP': '*'},\r\n",
        "                     {'DEP': 'attr', 'POS': 'NOUN'}]\r\n",
        "matcher2_to_products.add('attr', None, pattern2_products)\r\n",
        "\r\n",
        "for micro_sent in matched_to_pattern2:\r\n",
        "  doc = nlp(micro_sent)\r\n",
        "  matches = matcher2_to_products(doc)\r\n",
        "  for match_id, start, end in matches:\r\n",
        "    span = doc[start:end]\r\n",
        "    if span.text not in products:\r\n",
        "      products.append(span.text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QmeFtxJI7kA"
      },
      "source": [
        "И те же самые действия с третьим шаблоном, где наименование товара является корнем дерева зависимостей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh82Qeqm5TSb"
      },
      "source": [
        "matcher3 = Matcher(nlp.vocab)\r\n",
        "matcher3.add('what+root', None, pattern3)\r\n",
        "\r\n",
        "matched_to_pattern3 = []\r\n",
        "\r\n",
        "for rev in reviews[:100000]:\r\n",
        "  doc = nlp(rev)\r\n",
        "  matches = matcher3(doc)\r\n",
        "  for match_id, start, end in matches:\r\n",
        "    span = doc[start:end]\r\n",
        "    matched_to_pattern3.append(span.text)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOfPW0zF5nlv",
        "outputId": "950e67dd-c63d-49f7-8c70-ff689df296fb"
      },
      "source": [
        "print(len(matched_to_pattern3))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "379\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vthdts8eQH4E"
      },
      "source": [
        "Видно, что этот шаблон не самый удачный, но тем не менее и оттуда вычленим упоминания:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54EfYXSW_VsT"
      },
      "source": [
        "matcher3_to_products = Matcher(nlp.vocab)\r\n",
        "pattern3_products = [{'DEP': 'compound', 'POS': 'NOUN', 'OP': '*'},\r\n",
        "                     {'DEP': 'ROOT', 'POS': 'NOUN'}]\r\n",
        "matcher3_to_products.add('root', None, pattern3_products)\r\n",
        "\r\n",
        "for micro_sent in matched_to_pattern3:\r\n",
        "  doc = nlp(micro_sent)\r\n",
        "  matches = matcher3_to_products(doc)\r\n",
        "  for match_id, start, end in matches:\r\n",
        "    span = doc[start:end]\r\n",
        "    if span.text not in products:\r\n",
        "      products.append(span.text)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loe9RURcJGbt"
      },
      "source": [
        "Упоминания товаров из отзывов извлечены. Посмотрим на случайные элементы итогового списка products: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN8kPS6ZW1Sh",
        "outputId": "877d952b-4b66-4408-ffc6-0d82cfc8a619"
      },
      "source": [
        "print('Количество упоминаний: ', len(products))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество упоминаний:  5741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqvtG4o0JTYV",
        "outputId": "12fa06fd-0078-4da7-8c9e-2c968e347dd9"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "for rand_ind in np.random.randint(len(products), size=30):\r\n",
        "  print(products[rand_ind])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wolf tshirt\n",
            "maternity product\n",
            "mustaches\n",
            "size travel kit\n",
            "face helmet\n",
            "infant size\n",
            "sice\n",
            "timepiece\n",
            "couple pairs\n",
            "pity\n",
            "hair extensions\n",
            "version\n",
            "pics\n",
            "costume accessory\n",
            "stuffs\n",
            "PIECE\n",
            "sapphire shade\n",
            "SET\n",
            "body suits\n",
            "quality suit\n",
            "compression shirt\n",
            "thigh slimmer\n",
            "redhead\n",
            "fit jacket\n",
            "cream color version\n",
            "foundation\n",
            "street clothes\n",
            "surfing\n",
            "running socks\n",
            "video\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1isH9lhJUCl"
      },
      "source": [
        "##3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jdp768RTBz9"
      },
      "source": [
        "Для коллокаций будем смотреть на левого соседа. Это более естесственно для английского языка (в именной группе сначала идут модификаторы) и наши шаблоны были составлены именно так.\r\n",
        "\r\n",
        "Для этого мы пройдемся по тем частям отзывов, что подошли под наши шаблоны"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0ZVP-5SUMWp"
      },
      "source": [
        "matched_to_patterns = matched_to_pattern1\r\n",
        "matched_to_patterns.extend(matched_to_pattern2)\r\n",
        "matched_to_patterns.extend(matched_to_pattern3)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-j5l0UkJVUq"
      },
      "source": [
        "ngrams = []\r\n",
        "\r\n",
        "for match in matched_to_patterns:\r\n",
        "  for ent in products:\r\n",
        "    if ent in match:\r\n",
        "      match_words = match.split()\r\n",
        "      ent_words = ent.split()\r\n",
        "      if ent_words[0] in match_words:\r\n",
        "        ent_ind = match_words.index(ent_words[0])\r\n",
        "        if ent_ind>0:\r\n",
        "          ngrams.append(' '.join(match_words[ent_ind-1:ent_ind+len(ent)]))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxcQsM3QgJRx"
      },
      "source": [
        "Посмотрим на случайные 30 n-грамм:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ7ojh-pfdtu",
        "outputId": "f767ce29-a675-48c0-8dc6-1bc09d228449"
      },
      "source": [
        "for rand_ind in np.random.randint(len(ngrams), size=30):\r\n",
        "  print(ngrams[rand_ind])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a bit roomy\n",
            "this bag\n",
            "these sweatpants\n",
            "another one\n",
            "a tee shirt\n",
            "military beret\n",
            "wear support hose\n",
            "this brand\n",
            "buying things\n",
            "this cami\n",
            "black boots\n",
            "perfect choice\n",
            "buy things\n",
            "a YODA COSTUME\n",
            "this shirt\n",
            "the side\n",
            "last minute thing\n",
            "my second squeem\n",
            "hooded sweatshirts\n",
            "the feature\n",
            "best sports bra\n",
            "these socks\n",
            "this hat\n",
            "the industry standard\n",
            "lightweight alternative\n",
            "roomy backpack\n",
            "this dress\n",
            "this kit\n",
            "a cotton tank\n",
            "new bras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RysjIUE9gMfi"
      },
      "source": [
        "Очень часто левым соседом наименования сущности является артикль, что скорее всего говорит об отсутствии какого-то модификатора, поэтому никакой значимой информации мы не теряем. Есть достаточно много интересных коллокаций, перейдем к их анализу"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luPimfNBgkmn"
      },
      "source": [
        "##4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOl1TUGKjyRk"
      },
      "source": [
        "Выделенные n-граммы различаются по длине, поэтому чтобы применять коллокационные метрики, надо их разделить. Посмотрим сначала, какое количество биграмм, триграмм и т.д. имеется:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz9xr-PlhtHh",
        "outputId": "54967db7-a7c3-4c82-bc0c-95cf122ec787"
      },
      "source": [
        "lengthes = {2: 0, 3: 0, 4: 0, 'more': 0}\r\n",
        "for x in ngrams:\r\n",
        "  if len(x.split())>4:\r\n",
        "    lengthes['more'] += 1\r\n",
        "  else:\r\n",
        "    lengthes[len(x.split())] += 1\r\n",
        "\r\n",
        "lengthes"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2: 51253, 3: 11600, 4: 1225, 'more': 111}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxMDpWN1kCQ8"
      },
      "source": [
        "В основном биграммы и триграммы, а а также немного квадграмм, которые не будем брать во внимание. Разделим би- и триграммы на отдельные списки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpFZifMEkRzX"
      },
      "source": [
        "bigrams = []\r\n",
        "trigrams = []\r\n",
        "\r\n",
        "for x in ngrams:\r\n",
        "  if len(x.split()) == 2:\r\n",
        "    bigrams.append(x.split())\r\n",
        "  elif len(x.split()) == 3:\r\n",
        "    trigrams.append(x.split())\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zH4pyJWiyQ1"
      },
      "source": [
        "import nltk \r\n",
        "from nltk.collocations import *\r\n",
        "\r\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\r\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\r\n",
        "\r\n",
        "finder2 = BigramCollocationFinder.from_documents(bigrams)\r\n",
        "finder3 = TrigramCollocationFinder.from_documents(trigrams)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie69g-DDHJyf"
      },
      "source": [
        "Применим частотные фильтры: для биграм как минимум три упоминания (так как их больше), для триграм - два "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM7-CfPagmbE"
      },
      "source": [
        "finder2.apply_freq_filter(3)\r\n",
        "finder3.apply_freq_filter(2)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gRltxn1HRA_"
      },
      "source": [
        "Получим топ-20 ранжированных коллокаций различными способами:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQe7oCOfAIrN",
        "outputId": "46cc8ce5-a649-4554-c094-5b5beaaa6b3c"
      },
      "source": [
        "finder2.nbest(bigram_measures.student_t, 20)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 'size'),\n",
              " ('this', 'shirt'),\n",
              " ('this', 'bra'),\n",
              " ('these', 'socks'),\n",
              " ('this', 'bag'),\n",
              " ('this', 'product'),\n",
              " ('this', 'item'),\n",
              " ('a', 'pair'),\n",
              " ('another', 'one'),\n",
              " ('this', 'costume'),\n",
              " ('this', 'hat'),\n",
              " ('another', 'pair'),\n",
              " ('these', 'pants'),\n",
              " ('the', 'way'),\n",
              " ('these', 'jeans'),\n",
              " ('a', 'medium'),\n",
              " ('the', 'fact'),\n",
              " ('this', 'dress'),\n",
              " ('the', 'color'),\n",
              " ('this', 'jacket')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib3ovWUNHcmQ"
      },
      "source": [
        "Нет сомнений, что Критерий Стьюдента ранжирует высокими действительно часто встречаемые коллокации, потому что существительные сопровождаются артиклем. Также можно заметить, что среди топа немного \"мусора\" - только size, way, medium, fact и color. Но эти коллокации не являются информативными для отчета, поэтому эта метрика не подходит для конкретной задачи"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi6AjSq7BLxN",
        "outputId": "e5fc7bf8-37f8-4043-d89f-27e8ed6ea2dc"
      },
      "source": [
        "finder2.nbest(bigram_measures.pmi, 20)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('truck', 'driver'),\n",
              " ('arch', 'supports'),\n",
              " ('conversation', 'starter'),\n",
              " ('lottery', 'ticket'),\n",
              " ('olive', 'drab'),\n",
              " ('ear', 'muffs'),\n",
              " ('THESE', 'GLASSES'),\n",
              " ('THESE', 'SOCKS'),\n",
              " ('rave', 'reviews'),\n",
              " ('eel', 'skin'),\n",
              " ('eye', 'catcher'),\n",
              " ('fair', 'share'),\n",
              " ('star', 'wars'),\n",
              " ('prenatal', 'cradle'),\n",
              " ('rash', 'guard'),\n",
              " ('false', 'advertising'),\n",
              " ('thigh', 'highs'),\n",
              " ('gold', 'toes'),\n",
              " ('bouncy', 'seat'),\n",
              " ('super', 'heros')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFT0VLNwIEOb"
      },
      "source": [
        "У метрики PMI есть явная склонность к высокому ранжированию коллокаций, состоящих из двух существительных (в рамках этого датасета и моего метода). Причем среди топ-20 очень мало того, что относится к названиям продуктов. По сравнению с предыдущей метрикой, где лишнее было достаточно просто почистить руками, здесь высоко ранжируется много всего странного. А явные названия товаров высоко ранжированы только с детерминантом this. Эта метрика точно не подойдет для отчета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSve4_xmBkHl",
        "outputId": "5287b527-d707-4d75-943c-ab102104f48a"
      },
      "source": [
        "finder2.nbest(bigram_measures.chi_sq, 20)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('truck', 'driver'),\n",
              " ('rash', 'guard'),\n",
              " ('ear', 'muffs'),\n",
              " ('false', 'advertising'),\n",
              " ('star', 'wars'),\n",
              " ('arch', 'supports'),\n",
              " ('conversation', 'starter'),\n",
              " ('leg', 'warmers'),\n",
              " ('thigh', 'highs'),\n",
              " ('receiving', 'blankets'),\n",
              " ('card', 'holder'),\n",
              " ('eel', 'skin'),\n",
              " ('eye', 'catcher'),\n",
              " ('lottery', 'ticket'),\n",
              " ('olive', 'drab'),\n",
              " ('many', 'compliments'),\n",
              " ('super', 'heros'),\n",
              " ('THESE', 'GLASSES'),\n",
              " ('THESE', 'SOCKS'),\n",
              " ('rave', 'reviews')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6j8a9HXI9x2"
      },
      "source": [
        "Хи-квадрат выдает результат, похожий на выдачу ранжирования с метрикой PMI. По крайней мере набор биграмм такой же, немного отличается порядок. Опять же высоко ранжированы какие-то составные названия (rash guard, например, это наименование спортивной кофты). Но нет никаких коллокаций с оценочными прилагательными, и, как и выше было написано, много того, что к продуктам не относится совсем."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L388mXGJd5Q"
      },
      "source": [
        "Наблюдаем, что биграммы не дают много информации о том, как пользователи относятся к тому или иному товару. Этому не благоприятствует наличие у каждой ИГ артикля , т.е. надо было их игнорировать, а также сложные составные названия. \r\n",
        "\r\n",
        "Возможно, триграммы подойдут под формат отчета лучше:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9VvSUZeCUzs",
        "outputId": "d8c64ee8-dd72-477b-bf3c-3f265439745e"
      },
      "source": [
        "finder3.nbest(trigram_measures.student_t, 20)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'sports', 'bra'),\n",
              " ('a', 'good', 'product'),\n",
              " ('a', 'second', 'pair'),\n",
              " ('my', 'second', 'pair'),\n",
              " ('a', 'quality', 'product'),\n",
              " ('the', 'right', 'size'),\n",
              " ('a', 'second', 'one'),\n",
              " ('best', 'sports', 'bra'),\n",
              " ('a', 'sports', 'bra'),\n",
              " ('a', 'size', 'medium'),\n",
              " ('a', 'good', 'buy'),\n",
              " ('this', 'back', 'pack'),\n",
              " ('is', 'light', 'weight'),\n",
              " ('a', 'good', 'value'),\n",
              " ('the', 'second', 'time'),\n",
              " ('is', 'good', 'quality'),\n",
              " ('this', 'money', 'belt'),\n",
              " ('the', 'second', 'pair'),\n",
              " ('these', 'boxer', 'briefs'),\n",
              " ('my', 'third', 'pair')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaRo5PAtJ73c"
      },
      "source": [
        "Как и в случае с биграммами, высоко ранжируются коллокации с артиклями, которые не дают нам никакой информации. Однако уже есть прилагательные, но, из оценочного - только good. Другие относятся скорее к характеристикам самого товара, или являются порядковыми числительными."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuRvT9MDCwvo",
        "outputId": "5e522814-d1fb-44e2-8814-9f73621b84dd"
      },
      "source": [
        "finder3.nbest(trigram_measures.pmi, 20)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hanky', 'panky', 'thongs'),\n",
              " ('recycled', 'drink', 'boxes'),\n",
              " ('wally', 'world', 'specials'),\n",
              " ('above', 'ground', 'pool'),\n",
              " ('disposable', 'foil', 'pan'),\n",
              " ('witch', 'push', 'ins'),\n",
              " ('rough', 'kid', 'son'),\n",
              " ('coated', 'wire', 'hangers'),\n",
              " ('cheaper', 'knock', 'offs'),\n",
              " ('happy', 'tree', 'friends'),\n",
              " ('heart', 'rate', 'monitor'),\n",
              " ('darth', 'vader', 'lightsaber'),\n",
              " ('YOUR', 'PRODUCTS', 'EVER'),\n",
              " ('advertised', 'neon', 'colors'),\n",
              " ('avid', 'poker', 'player'),\n",
              " ('darker', 'wash', 'denimn'),\n",
              " ('true', 'visor', 'beanie'),\n",
              " ('upper', 'arm', 'decoration'),\n",
              " ('country', 'music', 'festival'),\n",
              " ('leap', 'pad', 'system')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rq_QPOuC4Pi",
        "outputId": "a2591cf3-8778-490d-ff8f-21c597eeb731"
      },
      "source": [
        "finder3.nbest(trigram_measures.chi_sq, 20)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hanky', 'panky', 'thongs'),\n",
              " ('recycled', 'drink', 'boxes'),\n",
              " ('wally', 'world', 'specials'),\n",
              " ('above', 'ground', 'pool'),\n",
              " ('disposable', 'foil', 'pan'),\n",
              " ('witch', 'push', 'ins'),\n",
              " ('rough', 'kid', 'son'),\n",
              " ('coated', 'wire', 'hangers'),\n",
              " ('cheaper', 'knock', 'offs'),\n",
              " ('happy', 'tree', 'friends'),\n",
              " ('heart', 'rate', 'monitor'),\n",
              " ('darth', 'vader', 'lightsaber'),\n",
              " ('avid', 'poker', 'player'),\n",
              " ('YOUR', 'PRODUCTS', 'EVER'),\n",
              " ('advertised', 'neon', 'colors'),\n",
              " ('darker', 'wash', 'denimn'),\n",
              " ('true', 'visor', 'beanie'),\n",
              " ('upper', 'arm', 'decoration'),\n",
              " ('leap', 'pad', 'system'),\n",
              " ('seal', 'skin', 'parkas')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDDUXl15K0Ls"
      },
      "source": [
        "Метрики PMI и хи-квадрат выдают похожие результаты, в топе одинаковые коллокации. Визуально они выглядят подходящими под отчет, потому что присутствуют не только банальные оценочные прилагательные. Но снова попадает достаточно много бессмыслицы, которая никак не относится к товарам одежды и аксессуаров. \r\n",
        "\r\n",
        "В целом, кажется, что t-test для триграмм показывает самый хороший и интерпретируемый результат, несмотря на то, что топ20 не является очень показательным (ниже топ-50 уже выглядит отлично, и действительно хорошо демонстрирует, понравился ли пользователю товар). Лишние сущности достаточно просто отследить на этом этапе, и убрать их вручную из рассмотрения на одном из предыдущих этапов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLN7KLnaLlMC",
        "outputId": "257a4c63-56ad-415c-fdaa-faf76b68aa9b"
      },
      "source": [
        "finder3.nbest(trigram_measures.student_t, 50)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'sports', 'bra'),\n",
              " ('a', 'good', 'product'),\n",
              " ('a', 'second', 'pair'),\n",
              " ('my', 'second', 'pair'),\n",
              " ('a', 'quality', 'product'),\n",
              " ('the', 'right', 'size'),\n",
              " ('a', 'second', 'one'),\n",
              " ('best', 'sports', 'bra'),\n",
              " ('a', 'sports', 'bra'),\n",
              " ('a', 'size', 'medium'),\n",
              " ('a', 'good', 'buy'),\n",
              " ('this', 'back', 'pack'),\n",
              " ('is', 'light', 'weight'),\n",
              " ('a', 'good', 'value'),\n",
              " ('the', 'second', 'time'),\n",
              " ('is', 'good', 'quality'),\n",
              " ('this', 'money', 'belt'),\n",
              " ('the', 'second', 'pair'),\n",
              " ('these', 'boxer', 'briefs'),\n",
              " ('my', 'third', 'pair'),\n",
              " ('a', 'good', 'size'),\n",
              " ('a', 'good', 'deal'),\n",
              " ('the', 'second', 'one'),\n",
              " ('good', 'quality', 'product'),\n",
              " ('credit', 'card', 'holder'),\n",
              " ('a', 'couple', 'pairs'),\n",
              " ('a', 'tank', 'top'),\n",
              " ('great', 'sports', 'bra'),\n",
              " ('my', 'favorite', 'bra'),\n",
              " ('a', 'good', 'thing'),\n",
              " ('a', 'good', 'idea'),\n",
              " ('a', 'good', 'bag'),\n",
              " ('these', 'sports', 'bras'),\n",
              " ('a', 'cute', 'costume'),\n",
              " ('this', 'money', 'clip'),\n",
              " ('my', 'second', 'one'),\n",
              " ('a', 'good', 'bra'),\n",
              " ('the', 'medium', 'size'),\n",
              " ('front', 'closure', 'bras'),\n",
              " ('first', 'sports', 'bra'),\n",
              " ('a', 'cup', 'size'),\n",
              " ('a', 'wolf', 'shirt'),\n",
              " ('a', 'black', 'one'),\n",
              " (\"It's\", 'good', 'quality'),\n",
              " ('these', 'leg', 'warmers'),\n",
              " ('this', 'bow', 'tie'),\n",
              " ('a', 'snug', 'fit'),\n",
              " ('this', 'duffel', 'bag'),\n",
              " ('only', 'sports', 'bra'),\n",
              " ('the', 'toddler', 'size')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGs32WYBDQ5p"
      },
      "source": [
        "##5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ417-6zDSsx"
      },
      "source": [
        "grouped_cols = {}\r\n",
        "\r\n",
        "for entity in products:\r\n",
        "  grouped_cols[entity] = []\r\n",
        "  for col in ngrams:\r\n",
        "    if entity in col:\r\n",
        "      if col not in grouped_cols[entity]:\r\n",
        "        grouped_cols[entity].append(col)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEJ4x1waEfdr",
        "outputId": "19f3f388-a07a-4db5-b74f-44d3acf187bb"
      },
      "source": [
        "random5 = np.random.randint(len(products), size=5)\r\n",
        "\r\n",
        "for i in random5:\r\n",
        "  print(products[i])\r\n",
        "  print('- - - - -')\r\n",
        "  for collocation in grouped_cols[products[i]]:\r\n",
        "    print(collocation)\r\n",
        "  print('- - - - -')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "polo shirt\n",
            "- - - - -\n",
            "polo shirt\n",
            "sleeved polo shirt\n",
            "polo shirts\n",
            "sleeveless polo shirts\n",
            "tight sleeveless polo shirts\n",
            "a white polo shirt\n",
            "white polo shirt\n",
            "good polo shirt\n",
            "a good polo shirt\n",
            "nice polo shirt\n",
            "- - - - -\n",
            "popularity\n",
            "- - - - -\n",
            "it's popularity\n",
            "- - - - -\n",
            "strap adjuster\n",
            "- - - - -\n",
            "seperate strap adjuster\n",
            "strap adjuster\n",
            "- - - - -\n",
            "light hat\n",
            "- - - - -\n",
            "light hat\n",
            "a light hat\n",
            "good light hat\n",
            "a good light hat\n",
            "- - - - -\n",
            "leg jeans\n",
            "- - - - -\n",
            "leg jeans\n",
            "slim leg jeans\n",
            "- - - - -\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
