{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве корпуса мною были взяты тексты рецептов на русском языке с сайта gastronom.ru. \n",
    "\n",
    "На странице каждого рецепта уже указаны ключевые слова в виде перечисления через запятую. Предполагаю, что авторы рецептов делают это самостоятельно, потому что количество и смысл выделенных ключевых слов от рецепта к рецепту может сильно разниться. \n",
    "\n",
    "Пример : https://www.gastronom.ru/recipe/4632/sup-harcho-iz-govjadiny-s-risom\n",
    "\n",
    "Я выбирала достаточно длинные рецепты, чтобы текст в них был не короче, чем 300 символов. Сам текст и ключевые слова доставала вручную (можно посмотреть код 'мини-корпус-рецептов.ipynb'). В выборку вошли 10 рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ризотто с морепродуктами — один из многочислен...</td>\n",
       "      <td>морепродукты, рис, мидии, креветки, итальянска...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nСуп харчо из говядины с рисом — знаменитое б...</td>\n",
       "      <td>говядина, рис, гранатовый сок, грузинская кухн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nРаньше такое блюдо, как фаршированный перец ...</td>\n",
       "      <td>говядина, баранина, мясной фарш, узбекская кух...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Хорошо приготовленный плов с курицей в казане ...</td>\n",
       "      <td>айва, плов, плов с курицей</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Как приятно в холодный пасмурный осенний день ...</td>\n",
       "      <td>паста, лазанья, итальянская кухня, говядина</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Ризотто с морепродуктами — один из многочислен...   \n",
       "1  \\nСуп харчо из говядины с рисом — знаменитое б...   \n",
       "2  \\nРаньше такое блюдо, как фаршированный перец ...   \n",
       "3  Хорошо приготовленный плов с курицей в казане ...   \n",
       "4  Как приятно в холодный пасмурный осенний день ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  морепродукты, рис, мидии, креветки, итальянска...  \n",
       "1  говядина, рис, гранатовый сок, грузинская кухн...  \n",
       "2  говядина, баранина, мясной фарш, узбекская кух...  \n",
       "3                         айва, плов, плов с курицей  \n",
       "4        паста, лазанья, итальянская кухня, говядина  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = pd.read_csv('kw-recipes.csv')\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что примерный объем корпуса подходит под задание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4272"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(corpus['text']).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавлю свою собственную разметку по ключевым словам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['keywords_hand'] = [\n",
    "    'ризотто с морепродуктами, рис, морепродукты, итальянская кухня, оливковое масло, сухое вино',\n",
    "    'суп харчо, говядина с рисом, грузинская кухня, бульон, гранатовый сок, зелень, приправы',\n",
    "    'фаршированный перец, мясной фарш, жареные овощи, приправы',\n",
    "    'плов с курицей, узбекская кухня, казан, чеснок, рис, зирвак, мясо',\n",
    "    'итальянская кухня, лазанья, говядина, тесто, соус болоньезе, соус бешамель',\n",
    "    'морковный пирог, морковь, изюм, грецкие орехи, тесто, миксер, выпечка',\n",
    "    'кулич, тесто для кулича, глазурь, опара, миксер, выпечка',\n",
    "    'заливное из курицы, праздничный стол, бульон, курица, вареное яйцо',\n",
    "    'суп фо, вьетнамская кухня, рисовая лапша, овощи, бульон, утиная грудка',\n",
    "    'торт птичье молоко, тесто, бисквит, глазурь, выпечка, миксер,'  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем колонки с ключевыми словами из строк в списки для более удобной работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['keywords'] = corpus['keywords'].apply(lambda x: x.split(', '))\n",
    "corpus['keywords_hand'] = corpus['keywords_hand'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на пересечение моей и авторской разметки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['итальянская кухня' 'морепродукты' 'рис']\n",
      "['гранатовый сок' 'грузинская кухня']\n",
      "['мясной фарш' 'фаршированный перец']\n",
      "['плов с курицей']\n",
      "['говядина' 'итальянская кухня' 'лазанья']\n",
      "['выпечка' 'грецкие орехи' 'изюм' 'морковный пирог' 'морковь']\n",
      "['выпечка' 'кулич']\n",
      "['заливное из курицы' 'курица' 'праздничный стол']\n",
      "['рисовая лапша' 'суп фо' 'утиная грудка']\n",
      "['торт птичье молоко']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "intersects = corpus.apply(lambda x: np.intersect1d(x['keywords'],x['keywords_hand']), axis=1)\n",
    "for x in intersects:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В пересечение попали достаточно очевидные элементы текстов рецепта, например, почти везде попало название самого блюда, основные ингридиенты. А также можно увидеть, что и я и автор выделяли ключевыми либо национальную особенность (грузинская кухня) блюда либо способ приготовления (выпечка)\n",
    "\n",
    "Я также не включала в разметку все ингридиенты блюда (в отличие от оригинальной, где часто перечислены каждый овощ, каждая специя), не указывала какие-то праздники или события, с чем блюдо ассоциируется (например, ужин, новый год или пасха), поскольку этого прямого указания в тексте нет.\n",
    "\n",
    "Есть три текста, где пересечением является только одно слово, и в целом размер пересечения не очень большой (2-3 слова в среднем), поэтому в качестве эталона логичнее взять объединение, чтобы было больше материала для дальнейшей работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['etalon'] = corpus.apply(lambda x: np.union1d(x['keywords'],x['keywords_hand']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем сразу функцию предобработки, которая будет делить тексты на токены, приводить слова к нижнему регистру, убирать знаки пунктуации и совершать лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('russian')\n",
    "punkt = punctuation + '«»—…“”*№–'\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [_.lower() for _ in tokens if _.lower() not in punkt]\n",
    "    lemmas = [morph.parse(word)[0].normal_form for word in tokens]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-rake in c:\\users\\maria\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages (1.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install python-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('процесс приготовление', 4.0),\n",
       " ('пора пока', 4.0),\n",
       " ('кой случай', 3.666666666666667),\n",
       " ('ризотто', 1.75),\n",
       " ('сразу', 1.6666666666666667),\n",
       " ('морепродукт', 1.25),\n",
       " ('слегка', 1.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import RAKE\n",
    "\n",
    "rake = RAKE.Rake(stopwords)\n",
    "preprocessing(corpus['text'][0])\n",
    "#посмотрим на пример\n",
    "rake.run(preprocessing(corpus['text'][0]), maxWords=3, minFrequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['keywords_rake'] = corpus['text'].apply(lambda x: rake.run(preprocessing(x), maxWords=3, minFrequency=2))\n",
    "#вытащим из списка кортежей только сами слова\n",
    "corpus['keywords_rake'] = corpus['keywords_rake'].apply(lambda x: [i[0] for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ризотто', 0.3077494106629952),\n",
       " ('лука', 0.1563321954067641),\n",
       " ('чеснок раздавить', 0.13750577078587095),\n",
       " ('это', 0.13176819900799153),\n",
       " ('морепродукт', 0.12891262347222157),\n",
       " ('время', 0.12095052285931167),\n",
       " ('всыпать сухой рис', 0.11809484033176383),\n",
       " ('половник', 0.11659711360352569),\n",
       " ('должный', 0.11527155389251971),\n",
       " ('нужный', 0.11414439043149406),\n",
       " ('нужно', 0.11414439043149406),\n",
       " ('мина добавить', 0.11020769568870238),\n",
       " ('очень', 0.10970433143465408),\n",
       " ('точно следовать', 0.10616003250389303),\n",
       " ('запах', 0.10009118803543124),\n",
       " ('нарезать', 0.0991600271834301),\n",
       " ('заранее', 0.0906186966166167),\n",
       " ('общий', 0.0902252318548854),\n",
       " ('risotto', 0.0898258469844134),\n",
       " ('белый перец', 0.08969561880865534),\n",
       " ('итальянский блюдо кстати', 0.08918977464491791),\n",
       " ('приготовление', 0.0882251925961469),\n",
       " ('который', 0.08820660345658153),\n",
       " ('белые', 0.08784919408345128),\n",
       " ('пристальный', 0.08641003609035122),\n",
       " ('костный', 0.08641003609035065),\n",
       " ('колбасный', 0.0864100360903504),\n",
       " ('каждый', 0.0864100360903504),\n",
       " ('необходимый', 0.0863053972946325),\n",
       " ('вино', 0.08268226464064578),\n",
       " ('положить', 0.08227064598661932),\n",
       " ('лёгкий', 0.0816437593701241),\n",
       " ('легко', 0.0816437593701241),\n",
       " ('мидия', 0.08139472381069317),\n",
       " ('причём', 0.08018384099893718),\n",
       " ('частый', 0.0799616172685397),\n",
       " ('часть', 0.0799616172685397),\n",
       " ('солёный', 0.07688926387942643),\n",
       " ('солёность', 0.07688926387942643),\n",
       " ('бульон', 0.07657612784464822),\n",
       " ('кайенский', 0.07129836453251644)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from summa import keywords\n",
    "\n",
    "# посмотрим на примере\n",
    "keywords.keywords(preprocessing(corpus['text'][0]), language='russian', additional_stopwords=stopwords, scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['keywords_textrank'] = corpus['text'].apply(lambda x: keywords.keywords(preprocessing(x), \n",
    "                                                                     language='russian',\n",
    "                                                                     additional_stopwords=stopwords).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большую частотность, судя по всему, имеет слово *это*, но оно не имеет конкретного значения и входит в разряд служебных в рамках этой задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.append('это')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого текста в корпусе мы:\n",
    "\n",
    "создаем матрицу tf-idf (в данном случае, вектор) для униграмм, биграмм и трехграмм текста, сортируем вектор по убыванию значений и получаем индексы, по первым 10 индексам получаем сами слова из словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_tfidf = []\n",
    "vec = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3))\n",
    "vec.fit(corpus['text'])\n",
    "words = vec.get_feature_names()\n",
    "for t in corpus['text']:\n",
    "    X = vec.transform([preprocessing(t)])\n",
    "    words = vec.get_feature_names()\n",
    "    inds = np.flip(np.argsort(X.toarray())[::-1])[0][:10]\n",
    "    kw = [words[i] for i in inds]\n",
    "    kw_tfidf.append(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ризотто',\n",
       " 'рис',\n",
       " 'добавить',\n",
       " 'половник',\n",
       " 'раздавить',\n",
       " 'лука',\n",
       " 'время',\n",
       " 'чеснок',\n",
       " 'бульон',\n",
       " 'кстати']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['keywords_tfidf'] = kw_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы работать с синтаксисом и морфологией, использую библиотеку udpipe (я уже много раз ей пользовалась и мне кажется невозможным использовать ее так, как показали в семинаре через командную строку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 40616122 / 40616122"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "\n",
    "wget.download('https://rusvectores.org/static/models/udpipe_syntagrus.model')\n",
    "m = Model.load('udpipe_syntagrus.model')\n",
    "process_pipeline = Pipeline(m, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ключевых слов буду оставлять либо существительные либо именные группы существительное+прилагательное\n",
    "\n",
    "Для этого воспользуюсь представлением в виде деревьев от NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "\n",
    "\n",
    "def filtering(g):\n",
    "    \"\"\"\n",
    "    Функция, проходящаяся по всем узлам дерева и возвращает либо существительное либо\n",
    "    существительное и от него зависящее прилагательное (если такое есть)\"\"\"\n",
    "    result = []\n",
    "    for n in g.nodes:\n",
    "        if g.nodes[n]['ctag'] == 'NOUN':\n",
    "            if 'amod' in g.nodes[n]['deps']:\n",
    "                adj_ind = g.nodes[n]['deps']['amod'][0]\n",
    "                result.append(' '.join((g.nodes[adj_ind]['lemma'].lower(), g.nodes[n]['lemma'].lower())))\n",
    "            else:\n",
    "                result.append(g.nodes[n]['lemma'].lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "\n",
    "# будем записывать отфильтрованные сущности для каждого текста\n",
    "filter_np = []\n",
    "# проходимся по каждому тексту\n",
    "for text in corpus['text']:\n",
    "    entities = []\n",
    "    # делим текст на предложения, поскольку дерево строится только для предложений, а не всего текста\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        # обрабатываем предложение с помощью udpipe\n",
    "        processed = process_pipeline.process(sent)\n",
    "        # парсим полученную информацию и создаем на ее основе дерево\n",
    "        content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "        g = DependencyGraph(content, top_relation_label='root')\n",
    "        #сохраняем найденные элементы\n",
    "        entities += filtering(g)\n",
    "    filter_np.append(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось на примере текста про ризотто:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ризотто',\n",
       " 'морепродукт',\n",
       " 'многочисленный вариант',\n",
       " 'знаменитый блюдо',\n",
       " 'сравнение',\n",
       " 'сало',\n",
       " 'костный мозг',\n",
       " 'сыр',\n",
       " 'мясо',\n",
       " 'колбасный изделие',\n",
       " 'фасоль',\n",
       " 'главное',\n",
       " 'правильный морепродукт',\n",
       " 'счет',\n",
       " 'вкус',\n",
       " 'блюдо',\n",
       " 'аромат',\n",
       " 'достижение',\n",
       " 'идеальный результат',\n",
       " 'указание']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_np[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все хорошо!\n",
    "\n",
    "Теперь для каждого текста найдем пересечение отфильтрованных единиц и найденных ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus['filter'] = filter_np\n",
    "\n",
    "corpus['f_keywords_rake'] = corpus.apply(lambda x: np.intersect1d(x['keywords_rake'], x['filter']), axis=1)\n",
    "corpus['f_keywords_textrank'] = corpus.apply(lambda x: np.intersect1d(x['keywords_textrank'], x['filter']), axis=1)\n",
    "corpus['f_keywords_tfidf'] = corpus.apply(lambda x: np.intersect1d(x['keywords_tfidf'], x['filter']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_keywords_rake</th>\n",
       "      <th>f_keywords_textrank</th>\n",
       "      <th>f_keywords_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[морепродукт, ризотто]</td>\n",
       "      <td>[блюдо, бульон, вино, время, запах, лука, миди...</td>\n",
       "      <td>[бульон, время, лука, половник, ризотто, рис, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[говядина, харчо]</td>\n",
       "      <td>[бульон, вкус, говядина, кастрюля, корень, мин...</td>\n",
       "      <td>[бульон, огонь, рис, суп, харчо]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[мясо]</td>\n",
       "      <td>[кубик, луковица, минута, морковь, начинка, ов...</td>\n",
       "      <td>[начинка, перец, рис, сладкий перец, фарш, фар...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[курица, плов]</td>\n",
       "      <td>[айва, вода, зирвак, казан, курица, кусок, лук...</td>\n",
       "      <td>[вода, зирвак, казан, курица, мясо, огонь, пло...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[ингредиент, конец, кусочек, минута, размер, с...</td>\n",
       "      <td>[кусочек, слой, соус, тесто]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[форма]</td>\n",
       "      <td>[духовка, изюм, корнеплод, лопаточка, минута, ...</td>\n",
       "      <td>[морковный пирог, морковь, пирог]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[кулич, форма]</td>\n",
       "      <td>[дрожжи, духовка, кулич, литр, минута, мука, п...</td>\n",
       "      <td>[дрожжи, кулич, полотенце, тесто]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[балкон, бульон, крышка, курица, небольшой ого...</td>\n",
       "      <td>[бульон, вода, время, желатин, кастрюля, колич...</td>\n",
       "      <td>[бульон, вода, курица, литр, ложка, мясо, яйцо]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[чеснок]</td>\n",
       "      <td>[бульон, бутон, говядина, грудка, жир, кастрюл...</td>\n",
       "      <td>[бульон, кипение, огонь, чеснок]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[форма, холодильник]</td>\n",
       "      <td>[бисквит, год, конфета, крахмал, крем, масса, ...</td>\n",
       "      <td>[крем, масса, молоко, основа, торт]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     f_keywords_rake  \\\n",
       "0                             [морепродукт, ризотто]   \n",
       "1                                  [говядина, харчо]   \n",
       "2                                             [мясо]   \n",
       "3                                     [курица, плов]   \n",
       "4                                                 []   \n",
       "5                                            [форма]   \n",
       "6                                     [кулич, форма]   \n",
       "7  [балкон, бульон, крышка, курица, небольшой ого...   \n",
       "8                                           [чеснок]   \n",
       "9                               [форма, холодильник]   \n",
       "\n",
       "                                 f_keywords_textrank  \\\n",
       "0  [блюдо, бульон, вино, время, запах, лука, миди...   \n",
       "1  [бульон, вкус, говядина, кастрюля, корень, мин...   \n",
       "2  [кубик, луковица, минута, морковь, начинка, ов...   \n",
       "3  [айва, вода, зирвак, казан, курица, кусок, лук...   \n",
       "4  [ингредиент, конец, кусочек, минута, размер, с...   \n",
       "5  [духовка, изюм, корнеплод, лопаточка, минута, ...   \n",
       "6  [дрожжи, духовка, кулич, литр, минута, мука, п...   \n",
       "7  [бульон, вода, время, желатин, кастрюля, колич...   \n",
       "8  [бульон, бутон, говядина, грудка, жир, кастрюл...   \n",
       "9  [бисквит, год, конфета, крахмал, крем, масса, ...   \n",
       "\n",
       "                                    f_keywords_tfidf  \n",
       "0  [бульон, время, лука, половник, ризотто, рис, ...  \n",
       "1                   [бульон, огонь, рис, суп, харчо]  \n",
       "2  [начинка, перец, рис, сладкий перец, фарш, фар...  \n",
       "3  [вода, зирвак, казан, курица, мясо, огонь, пло...  \n",
       "4                       [кусочек, слой, соус, тесто]  \n",
       "5                  [морковный пирог, морковь, пирог]  \n",
       "6                  [дрожжи, кулич, полотенце, тесто]  \n",
       "7    [бульон, вода, курица, литр, ложка, мясо, яйцо]  \n",
       "8                   [бульон, кипение, огонь, чеснок]  \n",
       "9                [крем, масса, молоко, основа, торт]  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[['f_keywords_rake', 'f_keywords_textrank', 'f_keywords_tfidf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за того, что RAKE выделил не очень много ключевых слов, и какие-то из них отфильтровались, то их осталось совсем мало\n",
    "\n",
    "В результатах, полученными другими алгоритмами, все хорошо, осталось много элементов, ушли ненужные глаголы, которые хоть и часто встречаются в текстах рецептов, ключевыми не являются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функции, которые будут считать необходимые метрики\n",
    "\n",
    "\n",
    "precision - отношение верно найденных алгоритмом ключевых слов ко всем найденным алгоритмом ключевым словам\n",
    "\n",
    "\n",
    "recall - отношение верно найденных алгоритмом ключевых слов ко всем ключевым словам, которые должны были быть найдены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(kw_etalon, kw_pred):\n",
    "    if len(kw_pred)==0:\n",
    "        return 0\n",
    "    return np.intersect1d(kw_etalon, kw_pred).shape[0]/len(kw_pred)\n",
    "\n",
    "\n",
    "def recall(kw_etalon, kw_pred):\n",
    "    if len(kw_etalon)==0:\n",
    "        return 0\n",
    "    return np.intersect1d(kw_etalon, kw_pred).shape[0]/len(kw_etalon)\n",
    "\n",
    "\n",
    "def f1(kw_etalon, kw_pred):\n",
    "    p = precision(kw_etalon, kw_pred)\n",
    "    r = recall(kw_etalon, kw_pred)\n",
    "    if p==0 or r==0:\n",
    "        return 0\n",
    "    return 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще необходимо учесть, что выделенные мной и авторами рецептов ключевые слова не стоят в начальной форме и содержат стоп-слова (типа \"плов с курицей\"). Выделенные алгоритмами списки слов этого не содержат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_cleaning(words):\n",
    "    clean_words = []\n",
    "    for ngram in words:\n",
    "        new_ngram = []\n",
    "        for word in ngram.split():\n",
    "            if word.lower() not in stopwords:\n",
    "                new_ngram.append(morph.parse(word)[0].normal_form)\n",
    "        clean_words.append(' '.join(new_ngram))\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['плов курица']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_cleaning(['плов с курицей'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики для каждого текста нашего корпуса и усредним:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for RAKE:  0.205\n",
      "Recall for RAKE:  0.08688034188034188\n",
      "f1-score for RAKE:  0.11414427076191783\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision for RAKE with filters:  0.3333333333333333\n",
      "Recall for RAKE with filters:  0.07085470085470086\n",
      "f1-score for RAKE with filters:  0.11373737373737373\n"
     ]
    }
   ],
   "source": [
    "print('Precision for RAKE: ', corpus.apply(lambda x: precision(kw_cleaning(x['etalon']), x['keywords_rake']), axis=1).mean())\n",
    "print('Recall for RAKE: ', corpus.apply(lambda x: recall(kw_cleaning(x['etalon']), x['keywords_rake']), axis=1).mean())\n",
    "print('f1-score for RAKE: ', corpus.apply(lambda x: f1(kw_cleaning(x['etalon']), x['keywords_rake']), axis=1).mean())\n",
    "print('\\n\\n\\n')\n",
    "print('Precision for RAKE with filters: ', corpus.apply(lambda x: precision(kw_cleaning(x['etalon']), x['f_keywords_rake']), axis=1).mean())\n",
    "print('Recall for RAKE with filters: ', corpus.apply(lambda x: recall(kw_cleaning(x['etalon']), x['f_keywords_rake']), axis=1).mean())\n",
    "print('f1-score for RAKE with filters: ', corpus.apply(lambda x: f1(kw_cleaning(x['etalon']), x['f_keywords_rake']), axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for TextRank:  0.07503353290422646\n",
      "Recall for TextRank:  0.2967959817959818\n",
      "f1-score for TextRank:  0.1179158796595053\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision for TextRank with filters:  0.20357975357975358\n",
      "Recall for TextRank with filters:  0.2658436008436008\n",
      "f1-score for TextRank with filters:  0.22324645821576303\n"
     ]
    }
   ],
   "source": [
    "print('Precision for TextRank: ', corpus.apply(lambda x: precision(kw_cleaning(x['etalon']), x['keywords_textrank']), axis=1).mean())\n",
    "print('Recall for TextRank: ', corpus.apply(lambda x: recall(kw_cleaning(x['etalon']), x['keywords_textrank']), axis=1).mean())\n",
    "print('f1-score for TextRank: ', corpus.apply(lambda x: f1(kw_cleaning(x['etalon']), x['keywords_textrank']), axis=1).mean())\n",
    "print('\\n\\n\\n')\n",
    "print('Precision for TextRank with filters: ', corpus.apply(lambda x: precision(kw_cleaning(x['etalon']), x['f_keywords_textrank']), axis=1).mean())\n",
    "print('Recall for TextRank with filters: ', corpus.apply(lambda x: recall(kw_cleaning(x['etalon']), x['f_keywords_textrank']), axis=1).mean())\n",
    "print('f1-score for TextRank with filters: ', corpus.apply(lambda x: f1(kw_cleaning(x['etalon']), x['f_keywords_textrank']), axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for TF-IDF:  0.22999999999999998\n",
      "Recall for TF-IDF:  0.22425269175269177\n",
      "f1-score for TF-IDF:  0.22514770224017772\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision for TF-IDF with filters:  0.40436507936507937\n",
      "Recall for TF-IDF with filters:  0.20822705072705072\n",
      "f1-score for TF-IDF with filters:  0.26381428169353865\n"
     ]
    }
   ],
   "source": [
    "print('Precision for TF-IDF: ', corpus.apply(lambda x: precision(kw_cleaning(x['etalon']), x['keywords_tfidf']), axis=1).mean())\n",
    "print('Recall for TF-IDF: ', corpus.apply(lambda x: recall(kw_cleaning(x['etalon']), x['keywords_tfidf']), axis=1).mean())\n",
    "print('f1-score for TF-IDF: ', corpus.apply(lambda x: f1(kw_cleaning(x['etalon']), x['keywords_tfidf']), axis=1).mean())\n",
    "print('\\n\\n\\n')\n",
    "print('Precision for TF-IDF with filters: ', corpus.apply(lambda x: precision(kw_cleaning(x['etalon']), x['f_keywords_tfidf']), axis=1).mean())\n",
    "print('Recall for TF-IDF with filters: ', corpus.apply(lambda x: recall(kw_cleaning(x['etalon']), x['f_keywords_tfidf']), axis=1).mean())\n",
    "print('f1-score for TF-IDF with filters: ', corpus.apply(lambda x: f1(kw_cleaning(x['etalon']), x['f_keywords_tfidf']), axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одну из проблем автоматических подходов к выделению ключевых слов мы решили - отфильтровали нужное с помощью морфосинтаксических признаков. Поскольку алгоритмы учитывают частоту встречаемости, то не обращает внимания на части речи (только в textrank от gensim можно сразу передать pos-фильтры). Это улучшило результаты достаточно значительно, потому что в рецептах часто встречаются различные глаголы действия (типа *перемешайте*, *посолите* и т.д.)\n",
    "\n",
    "\n",
    "Также и мной и авторами рецептов в списки ключевых слов были включены, по сути, заголовки - полные названия блюд (торт птичье молоко). Алгоритмы этого не делают и делят это значимое словосочетание на различные единицы. Наверное, чтобы этого избежать, надо каким-то образом повысить вес именно этого словосочетания вручную\n",
    "\n",
    "\n",
    "Основная проблема, которая свойственна, наверное, всем алгоритмам, - трудности с обобщением. То есть, например, прочитав какой-либо рецепт любой человек способен сказать, это \"второе блюдо\", \"угощение на новый год\", или \"холодная закуска\", даже если этих слов в тексте нет? Поэтому подобного рода вещи в ключевые слова не включаются алгоритмами. Решить эту проблему достаточно трудно, нужна какая-то внешняя база знаний, которая будет приниматься во внимание\n",
    "\n",
    "\n",
    "Еще проблема которая может возникнуть, но именно в рамках моего датасета ее не было: алгоритм не включает в ключевые слова важные именованные сущности, например, имена или географические названия, потому что они, допустим, встретились только в вводном предложении. В таком случае нужно подключать инструменты для задачи NER и вручную приписывать им больший вес или автоматически заносить в ключевые слова\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
